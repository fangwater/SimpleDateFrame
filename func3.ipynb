{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function_name:resample_day\n",
    "args: \n",
    "- date\n",
    "- n_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,os.path\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import collections.abc\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = f\"2022-12-06\"\n",
    "date = pd.to_datetime(date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfq_multi_daily = pd.read_parquet(f'./hfq_multi.parquet').reindex(index=[date]).fillna(method='ffill').replace(0,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_day_ticker(ticker,date):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore')\n",
    "        temp_path = './20221206'\n",
    "        if os.path.exists(f'{temp_path}/{ticker}.parquet'):\n",
    "            aa = pd.read_parquet(f'{temp_path}/{ticker}.parquet')\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "        if aa.empty:\n",
    "            return pd.DataFrame()\n",
    "        # array of time_stramp, date + time tick\n",
    "        # can be faster\n",
    "        aa['datetime'] = pd.to_datetime(aa['date'].apply(str)+' '+aa['updatetime'].apply(str))\n",
    "        # select\n",
    "        aa = aa.loc[aa.tradp>0]\n",
    "        # \n",
    "        aa['tradamt'] = aa['tradv'] * aa['tradp']\n",
    "        aa_B = aa.loc[aa.bs == 'B']\n",
    "        aa_S = aa.loc[aa.bs == 'S']\n",
    "        # a lot of groupby sampleing at the same index\n",
    "        res = aa.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradp'].count().to_frame()\n",
    "        res.columns = ['cjbs']\n",
    "        res['bcjbs'] = aa_B.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradp'].count()\n",
    "        res['scjbs'] = aa_S.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradp'].count()\n",
    "        res['volume_nfq'] = aa.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradv'].sum()\n",
    "        res['bvolume_nfq'] = aa_B.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradv'].sum()\n",
    "        res['svolume_nfq'] = aa_S.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradv'].sum()\n",
    "        res['amount'] = aa.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradamt'].sum()\n",
    "        res['bamount'] = aa_B.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradamt'].sum()\n",
    "        res['samount'] = aa_S.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradamt'].sum()\n",
    "        res['closeprice_nfq'] = aa.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradp'].last()\n",
    "        res['openprice_nfq'] = aa.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradp'].first()\n",
    "        res['highprice_nfq'] = aa.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradp'].max()\n",
    "        res['lowprice_nfq'] = aa.groupby([pd.Grouper(key='datetime',freq=f'T',closed='right',label='right',dropna=False),'securityid'])['tradp'].min()\n",
    "        return res.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done 4996 out of 4996 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res = pd.concat(Parallel(n_jobs=16,verbose=5,pre_dispatch='all',batch_size=500,backend='loky')(delayed(resample_day_ticker)(ticker,date) for ticker in hfq_multi_daily.columns),axis=0,copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile, pstats, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "for ticker in hfq_multi_daily.columns[:1000]:\n",
    "    resample_day_ticker(ticker,date)\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = \"cumtime\"  # 仅适用于 3.6, 3.7 把这里改成常量了\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "pr.dump_stats(\"day_ticker.prof\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_hfq_multi(hfq_multi_daily,dates,period):\n",
    "    hfq_multi = hfq_multi_daily.reindex(hfq_multi_daily.index.union([hfq_multi_daily.index[-1] + relativedelta(days=+1)])).resample(f'{period}T',label='right').ffill()\n",
    "    hfq_multi = hfq_multi.loc[hfq_multi.index.normalize().isin(dates)]\n",
    "    hfq_multi = pd.concat([hfq_multi.between_time('09:30','11:30',inclusive='right'),hfq_multi.between_time('13:00','15:00',inclusive='right')],axis=0).sort_index()\n",
    "    hfq_multi.index.name='time'\n",
    "    return hfq_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampling hfq_mutls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend MultiprocessingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "print('resampling hfq_mutls')\n",
    "hfq_multis = Parallel(n_jobs=4,verbose=5,backend='multiprocessing')(delayed(resample_hfq_multi)(hfq_multi_daily,[date],period) for period in [1,5,15,30])\n",
    "hfq_multis.append(hfq_multi_daily)\n",
    "hfq_multi_1min = hfq_multis[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = \"./result\"\n",
    "def save_feature(feature,ff,hfq_multi,period):\n",
    "    if feature == 'cjbs':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "    elif feature == 'bcjbs':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "    elif feature == 'scjbs':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "    elif feature == 'volume_nfq':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff/hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff/hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff = (ff/hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "    elif feature == 'bvolume_nfq':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff/hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff/hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff = (ff/hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "    elif feature == 'svolume_nfq':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff/hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff/hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff = (ff/hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "    elif feature == 'amount':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff_old = pd.read_parquet(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff_old = ff_old.loc[ff_old.index<ff.index[0]]\n",
    "            ff = pd.concat([ff_old,ff],axis=0,copy=False)\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "    elif feature == 'bamount':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff_old = pd.read_parquet(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff_old = ff_old.loc[ff_old.index<ff.index[0]]\n",
    "            ff = pd.concat([ff_old,ff],axis=0,copy=False)\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "    elif feature == 'samount':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').sum().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff_old = pd.read_parquet(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff_old = ff_old.loc[ff_old.index<ff.index[0]]\n",
    "            ff = pd.concat([ff_old,ff],axis=0,copy=False)\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "    elif feature == 'closeprice_nfq':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').last().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff*hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff*hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').last().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff = (ff*hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "    elif feature == 'openprice_nfq':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').first().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff*hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff*hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').first().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff = (ff*hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "    elif feature == 'highprice_nfq':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').max().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff*hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff*hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').max().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff = (ff*hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "    elif feature == 'lowprice_nfq':\n",
    "        if period != 'daily' and period != 1:\n",
    "            ff = ff.resample(f'{period}T',label='right',closed='right').min().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff*hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        elif period == 1:\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff=(ff*hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')\n",
    "        else:\n",
    "            ff = ff.resample(f'D').min().reindex(hfq_multi.index)\n",
    "            ff[np.isnan(hfq_multi)] = np.nan\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature}.csv.bz2',compression='bz2')\n",
    "            ff = (ff*hfq_multi)\n",
    "            ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{feature[:-4]}.csv.bz2',compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving hfq_mutls\n"
     ]
    }
   ],
   "source": [
    "print('saving hfq_mutls')\n",
    "for i,period in enumerate(['1min','5min','15min','30min','daily']):\n",
    "    if not os.path.exists(f'{result_path}/{period}/{date.strftime(\"%Y%m%d\")}/'):\n",
    "        os.makedirs(f'{result_path}/{period}/{date.strftime(\"%Y%m%d\")}/')\n",
    "        print(f'{result_path}/{period}min/{date.strftime(\"%Y%m%d\")}/')\n",
    "    hfq_multis[i].to_csv(f'{result_path}/{period}/{date.strftime(\"%Y%m%d\")}/hfq_multi.csv.bz2',compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving hfq_mutls\n"
     ]
    }
   ],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "print('saving hfq_mutls')\n",
    "for i,period in enumerate(['1min','5min','15min','30min','daily']):\n",
    "    if not os.path.exists(f'{result_path}/{period}/{date.strftime(\"%Y%m%d\")}/'):\n",
    "        os.makedirs(f'{result_path}/{period}/{date.strftime(\"%Y%m%d\")}/')\n",
    "        print(f'{result_path}/{period}min/{date.strftime(\"%Y%m%d\")}/')\n",
    "    hfq_multis[i].to_csv(f'{result_path}/{period}/{date.strftime(\"%Y%m%d\")}/hfq_multi.csv.bz2',compression='bz2')\n",
    "\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = \"cumtime\"  # 仅适用于 3.6, 3.7 把这里改成常量了\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "pr.dump_stats(\"save_feature.prof\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivoting(feature,df,hfq_multi_1min):\n",
    "    tmp = df.pivot(index='datetime',columns='securityid',values=feature)\n",
    "    ff = pd.concat([tmp.between_time('09:30','11:30',inclusive='right'),tmp.between_time('13:00','15:00',inclusive='right')],axis=0).sort_index()\n",
    "    ff = ff.reindex(hfq_multi_1min.index,columns=hfq_multi_1min.columns)\n",
    "    if feature in ['cjbs','bcjbs','scjbs','volume_nfq','bvolume_nfq','svolume_nfq','amount','bamount','samount']:\n",
    "        ff = ff.fillna(0)\n",
    "    elif feature == 'closeprice_nfq':\n",
    "        ff = ff.fillna(method='ffill')\n",
    "    ff[np.isnan(hfq_multi_1min)] = np.nan\n",
    "    ff.index.name = 'time'\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vwap(amount,volume_nfq,hfq_multi,period,name):\n",
    "    fns = []\n",
    "    if period != 'daily':\n",
    "        amount = amount.resample(f'{period}T',label='right',closed='right').sum().reindex(hfq_multi.index)\n",
    "        volume_nfq = volume_nfq.resample(f'{period}T',label='right',closed='right').sum().reindex(hfq_multi.index)\n",
    "        ff = (amount/volume_nfq).replace([np.inf,-np.inf],np.nan)\n",
    "        ff[np.isnan(hfq_multi)] = np.nan\n",
    "        ff.index.name='time'\n",
    "        ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{name}_nfq.csv.bz2',compression='bz2')\n",
    "        fns.append(ff)\n",
    "        volume = volume_nfq/hfq_multi\n",
    "        ff = (amount/volume).replace([np.inf,-np.inf],np.nan)\n",
    "        ff[np.isnan(hfq_multi)] = np.nan\n",
    "        ff.index.name='time'\n",
    "        ff.to_csv(f'{result_path}/{period}min/{ff.index[-1].strftime(\"%Y%m%d\")}/{name}.csv.bz2',compression='bz2')\n",
    "        fns.append(ff)\n",
    "    else:\n",
    "        amount = amount.resample(f'D').sum().reindex(hfq_multi.index)\n",
    "        volume_nfq = volume_nfq.resample(f'D').sum().reindex(hfq_multi.index)\n",
    "        ff = (amount/volume_nfq).replace([np.inf,-np.inf],np.nan)\n",
    "        ff[np.isnan(hfq_multi)] = np.nan\n",
    "        ff.index.name='time'\n",
    "        ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{name}_nfq.csv.bz2',compression='bz2')\n",
    "        fns.append(ff)\n",
    "        volume = volume_nfq/hfq_multi\n",
    "        ff = (amount/volume).replace([np.inf,-np.inf],np.nan)\n",
    "        ff[np.isnan(hfq_multi)] = np.nan\n",
    "        ff.index.name='time'\n",
    "        ff.to_csv(f'{result_path}/daily/{ff.index[-1].strftime(\"%Y%m%d\")}/{name}.csv.bz2',compression='bz2')\n",
    "        fns.append(ff)\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  2\n",
       "1  3\n",
       "2  4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.apply(lambda x:x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  2\n",
       "2  3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend MultiprocessingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   9 out of  13 | elapsed:    2.6s remaining:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done  13 out of  13 | elapsed:    3.1s finished\n"
     ]
    }
   ],
   "source": [
    "feature_cols = ['cjbs','bcjbs','scjbs','volume_nfq','bvolume_nfq','svolume_nfq','amount','bamount','samount','closeprice_nfq','openprice_nfq','highprice_nfq','lowprice_nfq']\n",
    "print('transforming features')\n",
    "features = Parallel(n_jobs=4,verbose=5,backend='multiprocessing')(delayed(pivoting)(feature,res[['datetime','securityid',feature]],hfq_multi_1min) for i,feature in enumerate(feature_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating vwaps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend MultiprocessingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   4 out of  15 | elapsed:    0.6s remaining:    1.7s\n",
      "[Parallel(n_jobs=16)]: Done   8 out of  15 | elapsed:    1.0s remaining:    0.9s\n",
      "[Parallel(n_jobs=16)]: Done  12 out of  15 | elapsed:    2.1s remaining:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done  15 out of  15 | elapsed:    6.6s finished\n"
     ]
    }
   ],
   "source": [
    "print('calculating vwaps')\n",
    "final = Parallel(n_jobs=16,verbose=5,backend='multiprocessing')(delayed(calc_vwap)(a,b,hfq_multi,period,name) for a,b,name in zip([features[6],features[7],features[8]],[features[3],features[4],features[5]],['vwap','bvwap','svwap']) for period,hfq_multi in zip([1,5,15,30,'daily'],hfq_multis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "for a,b,name in zip([features[6],features[7],features[8]],[features[3],features[4],features[5]],['vwap','bvwap','svwap']) for period,hfq_multi in zip([1,5,15,30,'daily'],hfq_multis):\n",
    "    calc_vwap(a,b,hfq_multi,period,name)\n",
    "\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = \"cumtime\"  # 仅适用于 3.6, 3.7 把这里改成常量了\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "pr.dump_stats(\"save_feature.prof\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hfq_multi_daily\n",
    "col:行情数据文件ID\n",
    "row:日期\n",
    "dates 选择一批日期\n",
    "period 采样精度\n",
    "\n",
    "[1]在原来日期上增加一天"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hfq_multi_daily.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
